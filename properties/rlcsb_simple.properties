#random.seed=1234567
run.name=rlcsbsimple
run.reset=true

# If set to "true" then substitutions present in property values will be enabled. Substitutions have the format $([key]), where [key] is the key of another property.
substitution.enable=true

###########
# evolution
###########

num.runs=1
num.generations=2000
popul.size=1000

performance.target=1.0
performance.target.type=higher
# If greater than 1 then use an average of the best performance over this many generations.
performance.target.average=10

#false means mutation probabilities are applied to all possible places a mutation could occur
#true means probabilities apply to individual as a whole; only one topological mutation can occur per individual
#note that this applies only to topological mutations, not weight mutations
topology.mutation.classic=false


#classic=[0.01, 0.5], not classic=[0.0001,] dependent on pop size. 0.03
add.neuron.mutation.rate=0.01

# Mutation rate for operator that adds neurons anywhere in the network (as 
# opposed to regular add neuron operator that only adds them in place of 
# existing connections). Only works for topology.mutation.classic=false
add.neuron.anywhere.mutation.rate=$(add.neuron.mutation.rate)

#classic=[0.01, 0.5], not classic=[0.0001,] dependent on pop size. 0.4
add.connection.mutation.rate=0.2
#[0.01, 0.3]
remove.connection.mutation.rate=0.001
#only remove weights with magnitude smaller than this
remove.connection.max.weight=0.1

#[0.1, 0.8]
weight.mutation.rate=0.15
#[1.0, 2.0] dependent on (CPPN) weight.max/min?
weight.mutation.std.dev=0.3
# The amount to perturb weights by when generating the initial population. Default is weight.mutation.std.dev
#weight.mutation.std.dev.initial=0.01

# probability of perturbing generic parameters (only used for synapse and neuron model 
# parameter classes, see ann.transcriber.neuron.model.params.classes).
param.mutation.rate=0.1
param.mutation.std.dev=1
#param.mutation.std.dev.initial

#percent of individuals used as parents
survival.rate=0.3
#proportion of sexual (crossover) versus asexual reproduction.
crossover.proportion=0.5
# the probability that an individual produced by the crossover operator will be a candidate for having mutations applied to it (independent of other mutation probabilities).
crossover.mutate.probability=0.5

#[1, 5]
selector.elitism.min.specie.size=0
#percent of individuals from each species copied to next generation unchanged
selector.elitism.proportion=0.2
#min number to select from a species (if it has size >=  selector.elitism.min.specie.size), default is the number of objectives defined by the fitness function.
selector.elitism.min.to.select=0
# The NaturalSelector to use to perform the parent (and elite) selection. Default is com.anji.integration.SimpleSelector.
selector.class=com.ojcoleman.ahni.misc.NSGAIISelector
selector.min.generations=0
selector.max.stagnant.generations=100000
selector.speciated.fitness=true


############
# speciation
############
#species distance factors
#c1, excess genes factor [1.0, 2.0]
chrom.compat.excess.coeff=1
#c2, disjoint genes factor [1.0, 2.0]
chrom.compat.disjoint.coeff=1
#c3, Weight difference factor [0.2, 3.0]
chrom.compat.common.coeff=1

chrom.compat.mismatch_use_values=true

speciation.class=com.anji.neat.SpeciationStrategyKMeans
# initial compatibility threshold [0.1, 4.0], relative to c#
#speciation.threshold=16
#speciation.threshold.min=0
#speciation.threshold.max=100000
# Target number of species, default is popul.size ^ 0.6 (bit more than square root)
speciation.target=32


##################
# fitness function
##################
fitness_function.class=com.ojcoleman.ahni.experiments.csb.RLContinuousStateBased
#max threads to use for fitness evaluation (including transcription of genotype/cppn to phenotype/substrate)
#if value is <= 0 then the detected number of processor cores will be used
fitness.max_threads=0


#fitness.function.multi.class=com.ojcoleman.ahni.evaluation.novelty.GenericBehaviourEvaluator
#fitness.function.multi.class=com.ojcoleman.ahni.evaluation.mocostfunctions.BainNNConnectionCountCost
#fitness.function.multi.class=com.ojcoleman.ahni.evaluation.novelty.GenericBehaviourEvaluator, com.ojcoleman.ahni.evaluation.mocostfunctions.BainNNConnectionCountCost
#fitness.function.multi.class=com.ojcoleman.ahni.evaluation.mocostfunctions.CPPNSizeCost

#rlcsb=com.ojcoleman.ahni.experiments.csb.RLContinuousStateBased
#fitness.function.multi.class=$(rlcsb), $(rlcsb), $(rlcsb), $(rlcsb), $(rlcsb), $(rlcsb), $(rlcsb), $(rlcsb), $(rlcsb)

#fitness.function.multi.weighting=0.5, 0.5, 0.0001


### Experiment specific

# The class implementing the environment to use. Must extend com.ojcoleman.ahni.experiments.csb.Environment. Defaults to com.ojcoleman.ahni.experiments.csb.SimpleNavigationEnvironment.
#fitness.function.rlcss.environment.class=com.ojcoleman.ahni.experiments.csb.StaticTransformEnvironment

# The number of variables defining an environment state. If 2 then a map can be rendered of the agents progress through the environment.
fitness.function.rlcss.size=2

# The performance indicating when the environment difficulty should be increased as the current difficulty has been sufficiently mastered, probably only makes sense for fitness.function.rlcss.environment.replacerate > 0.
fitness.function.rlcss.difficulty.increase.performance=1
 
# The number of environments to evaluate candidates against. If fitness.function.rlcss.environment.replacerate > 0 then increasing this will provide a more accurate evaluation but take longer.
fitness.function.rlcss.environment.count=5

# The fraction of environments that should be replaced with new environments per generation. This is evaluated probabilistically.
fitness.function.rlcss.environment.replacerate=0

# The number of trials per environment. Can be same as fitness.function.rlcss.environment.count if fitness.function.rlcss.environment.replacerate=0
fitness.function.rlcss.trial.count=$(fitness.function.rlcss.environment.count)
#fitness.function.rlcss.trial.count=1

# If true enables novelty search for which behaviours are defined by the agent output for the first fitness.function.rlcss.noveltysearch.first_steps.record_count steps of each trial. Default is false.
fitness.function.rlcss.noveltysearch.first_steps=false

# See fitness.function.rlcss.noveltysearch.first_steps. Default is 2 * fitness.function.rlcss.size ^ 2.
#fitness.function.rlcss.noveltysearch.first_steps.record_count=8

# If true enables novelty search for which behaviours are defined by the environment state at #fitness.function.rlcss.noveltysearch.even_distribution.record_count 
# equally spaced intervals between the beginning (exclusive) and end (inclusive) of each trial. Default is false.
fitness.function.rlcss.noveltysearch.even_distribution=true

# See fitness.function.rlcss.noveltysearch.even_distribution. Default is 1 if fitness.function.rlcss.trial.count > 1, else 4.
#fitness.function.rlcss.noveltysearch.even_distribution.record_count=2



#### For com.ojcoleman.ahni.experiments.csb.SimpleNavigationEnvironment and com.ojcoleman.ahni.experiments.csb.StaticTransformEnvironment

# The initial number of obstacles that should be included. Default is 0.
fitness.function.rlcss.obstacles.initial=0

# The amount to increase the obstacle count when environments with the current value have been sufficiently mastered. If the value is followed by an "x" then the value is considered a factor (and so should be > 1). Default is 1.
fitness.function.rlcss.obstacles.delta=0

# The maximum amount to increase the obstacle count to. Default is lots.
fitness.function.rlcss.obstacles.maximum=$(fitness.function.rlcss.obstacles.initial)



######## Novelty search (global parameters)

# The number of nearest neighbours to consider when determining the sparseness in a region and so whether to add a new individual to the archive. Default is 30.
fitness.function.novelty.k=15

# The novelty threshold to determine whether an individual is novel enough to add to the archive. The novelty
# of an individual is always in the range [0, 1], thus the threshold should also be within this range. Default
# is 0.05. An alternative method where individuals are added probabilistically can be used by removing this option
# and setting fitness.function.novelty.add_probability > 0. This option is mutually exclusive with 
# fitness.function.novelty.add_probability
#fitness.function.novelty.threshold=0.4

# The minimum value to decrease the novelty threshold to (the threshold is slowly reduced if no individuals are
# added in a generation). Default is 0.05 * fitness.function.novelty.threshold.
#fitness.function.novelty.threshold.min=0.0025

# The probability for each individual from the current generation that it will be added to the archive. For 
# example if the population size is 1000 and fitness.function.novelty.add_probability == 0.001, then on average
# one (randomly selected) individual will be added to the archive. This option is mutually exclusive with 
# fitness.function.novelty.threshold. Default is 0 (disabled, threshold method will be used instead).
fitness.function.novelty.add_probability=0.001


######## Generic novelty search fitness function (if included in fitness.function.multi.class)

# The number of sequences to test individuals on.
fitness.function.generic_novelty.sequence_count=8

# The number of output samples to record for each sequence.
fitness.function.generic_novelty.sample_count=4

# Output samples will be taken every [fitness.function.generic_novelty.sampling_interval]th step in the sequence. Default is 1 (take a sample every step).
fitness.function.generic_novelty.sampling_interval=4

# The minimum input value. Default is 0.
fitness.function.generic_novelty.input.min=-1

# The maximum input value. Default is 1.
fitness.function.generic_novelty.input.max=1


######## target connection count fitness function (if included in fitness.function.multi.class)

# The target proportion of synapses based on maximum possible number of synapses (calculated as number of neurons squared). Default is 0.
#fitness.function.connection_count_cost.target=0.2



################
# CPPN/AnjiNet #
################
#input and output size determined by fitness function settings
#stimulus.size=6
#response.size=6
initial.topology.activation=random
initial.topology.fully.connected=false
initial.topology.num.hidden.neurons=0
initial.topology.activation.input=linear
# Using an activation function with range [0, 1] or [-1, 1] causes the transcriber to scale the output to the substrate weight range, rather than truncating it to that range.
initial.topology.activation.output=linear
initial.topology.activation.random.allowed=sigmoid, gaussian, sine, absolute, ramp, linear, sign, multiply, divide
#initial.topology.activation.random.allowed=absolute, sigmoid-bipolar, gaussian, sine, reciprocal, multiply, divide, linear, and, or, xor, clamped-linear, ramp, power
#initial.topology.activation.random.allowed=absolute, sigmoid, gaussian, sine
#initial.topology.activation.random.probabilities=0.2, 1, 0.5, 0.5, 0.2, 0.1
bias.via.input=false

recurrent=disallowed
recurrent.cycles=1
#[1, 500]
# Set to 3 as this is the magnitude for one connection receiving an input of 1 to drive a (bipolar-)sigmoid or gaussian to saturation.
#weight.min=-20
weight.max=3


#####################
# HyperNEAT/BainNN #
#####################
#ann.transcriber.class=com.ojcoleman.ahni.transcriber.NEATTranscriberBain
ann.transcriber.class=com.ojcoleman.ahni.transcriber.HyperNEATTranscriberBain
#ann.transcriber.class=com.ojcoleman.ahni.transcriber.ESHyperNEATTranscriberBain
#ann.transcriber.bain.maxrecurrentcyclesearchlength=20
ann.transcriber.bain.executionmode=SEQ

ann.transcriber.neuron.model=com.ojcoleman.bain.neuron.rate.RisiModulatoryNeuronCollection
ann.transcriber.synapse.model=com.ojcoleman.bain.synapse.rate.RisiModulatorySynapseCollection
#ann.transcriber.neuron.model=com.ojcoleman.bain.neuron.rate.SigmoidBipolarNeuronCollection
#ann.transcriber.synapse.model=com.ojcoleman.bain.synapse.rate.FixedSynapseCollection

# Create CPPN outputs that set the parameters for each neuron.
ann.transcriber.neuron.model.params=modBias

# Specifies that instead of parameter values being defined individually for each
# neuron, there will be a set of parameter value collections which can be thought of as defining a class of
# neurons as determined by the specific parameter values (which will determine the behaviour of that class of
# neuron). Each neuron then references one of these collections/classes and has its parameters set
# accordingly. If this is set to 0 or not specified then each synapse should be assigned parameters values directly
# (e.g. via an output for each parameter from a CPPN in a HyperNEAT encoding scheme), if this is set to a value
# greater than 0 then it specifies the number of classes.
#ann.transcriber.neuron.model.params.classes=6

# Min and max values for model parameters.
ann.transcriber.neuron.model.params.min=-$(ann.transcriber.connection.weight.max)
ann.transcriber.neuron.model.params.max=$(ann.transcriber.connection.weight.max)
ann.transcriber.neuron.model.params.expression.threshold=0.1


# Create CPPN outputs that set the parameters for each synapse.
ann.transcriber.synapse.model.params=n,a,b,c,d

# See ann.transcriber.neuron.model.params.classes
ann.transcriber.synapse.model.params.classes=2

# Min and max values for model parameters.
ann.transcriber.synapse.model.params.min=-10,-1,-1,-1,-1
ann.transcriber.synapse.model.params.max=10,1,1,1,1
ann.transcriber.synapse.model.params.expression.threshold=0.2,0.1,0.1,0.1,0.1
#ann.transcriber.synapse.model.params.expression.threshold=0.1,0,0,0
# Two synapse types: regular = 0, modulatory = 1
# Separate weight outputs from CPPN will be used for each type. 
ann.transcriber.synapse.model.types=modulatory,0,1
# This parameter in the synapse model will be set to 0 if the connection should not be expressed. This is typically applied to a "learning rate" parameter.
ann.transcriber.synapse.model.plasticitydisableparam=n

ann.hyperneat.feedforward=false
#For networks with recurrent connections, the number of activation cycles to perform each time the substrate network is presented with new input and queried for its output.
#For this experiment the network can decide when it's ready by setting the last output to a value greater than 0.5. 
ann.hyperneat.cyclesperstep=1
ann.hyperneat.enablebias=true
ann.hyperneat.includedelta=true
ann.hyperneat.includeangle=false
ann.hyperneat.useinputlayerencoding=true

ann.hyperneat.connection.expression.threshold=0
ann.hyperneat.leo=true
ann.hyperneat.leo.threshold=0.2
ann.hyperneat.leo.threshold.factordistance=true
ann.hyperneat.leo.threshold.directionalfactor=6,2,1
#ann.hyperneat.neo=true
#ann.hyperneat.neo.threshold=0

#ann.transcriber.connection.weight.min=-2
ann.transcriber.connection.weight.max=8

# input and output layer dimensions determined by fitness function
# set middle layers manually, corresponding to dimensionality of environment state?
#ann.hyperneat.width=f,$(fitness.function.rlcss.size),f
#ann.hyperneat.height=f,$(fitness.function.rlcss.size),f

ann.hyperneat.hiddenlayersize=2
ann.hyperneat.width=f,$(ann.hyperneat.hiddenlayersize),f
ann.hyperneat.height=f,$(ann.hyperneat.hiddenlayersize),f


ann.hyperneat.range.x=-1,1
ann.hyperneat.range.y=-1,1
ann.hyperneat.range.z=-1,1


# ES-HypernNEAT params.
ann.eshyperneat.iterations=1
ann.eshyperneat.depth.initial=1
ann.eshyperneat.depth.max=2
ann.eshyperneat.division.threshold=0.5
ann.eshyperneat.variance.threshold=0.03
ann.eshyperneat.band.threshold=0.3


#############
# persistence
#############
persistence.class=com.anji.persistence.FilePersistence
persistence.base.dir=./db
persist.enable=false
persist.all=false
persist.champions=false
persist.last=false
persist.load.genotype=false
id.file=./db/id.xml
neat.id.file=./db/neatid.xml

##############
# presentation
##############
presentation.generate=false
presentation.dir=./nevt

#########
# logging
#########
output.dir=../../temp/$(run.name)
# How often to produce a line in the log containing a brief summary of the current progress.
log.pergenerations=1
# Whether to log the champ to a text file and/or image. N < 0 indicates no logging, N=0 indicates 
# only at the end of evolution, N > 0 indicates every N generations and after evolution has finished.
log.champ.tostring=50
log.champ.toimage=50
#log.selector.nsgaii=true


# FileAppenders with the name RunLog receive special treatment: for each run the output will be directed to a file 
# with the name specified by log4j.appender.RunLog.File in the directory [output.dir]/[run number]/
#log4j.rootLogger=INFO, C, RunLog
log4j.rootLogger=INFO, C, RunLog
log4j.appender.C=org.apache.log4j.ConsoleAppender
log4j.appender.RunLog=org.apache.log4j.FileAppender
log4j.appender.RunLog.File=log.txt
log4j.appender.C.layout=org.apache.log4j.PatternLayout
log4j.appender.RunLog.layout=org.apache.log4j.PatternLayout
log4j.appender.C.layout.ConversionPattern=%-5p %m%x%n
log4j.appender.RunLog.layout.ConversionPattern=%-5p %m%x%n



#######################################
# parameter tuning via ParameterTuner #
#######################################

parametertuner.tune.5.prop=ann.transcriber.synapse.model.params.classes
parametertuner.tune.5.type=integer
parametertuner.tune.5.adjust.type=delta
parametertuner.tune.5.adjust.amount=2
parametertuner.tune.5.initial=6
parametertuner.tune.5.min=0
parametertuner.tune.5.max=10000

#parametertuner.tune.10.prop=add.neuron.mutation.rate
#parametertuner.tune.10.type=float
#parametertuner.tune.10.adjust.type=factor
#parametertuner.tune.10.adjust.amount=2
#parametertuner.tune.10.initial=0.01
#parametertuner.tune.10.min=0
#parametertuner.tune.10.max=1000

#parametertuner.tune.20.prop=add.connection.mutation.rate
#parametertuner.tune.20.type=float
#parametertuner.tune.20.adjust.type=factor
#parametertuner.tune.20.adjust.amount=2
#parametertuner.tune.20.initial=0.2
#parametertuner.tune.20.min=0
#parametertuner.tune.20.max=1000

parametertuner.tune.30.prop=param.mutation.rate
parametertuner.tune.30.type=float
parametertuner.tune.30.adjust.type=factor
parametertuner.tune.30.adjust.amount=2
parametertuner.tune.30.initial=0.5
parametertuner.tune.30.min=0
parametertuner.tune.30.max=1

parametertuner.tune.40.prop=param.mutation.std.dev
parametertuner.tune.40.type=float
parametertuner.tune.40.adjust.type=factor
parametertuner.tune.40.adjust.amount=2
parametertuner.tune.40.initial=0.5
parametertuner.tune.40.min=0
parametertuner.tune.40.max=100

#parametertuner.tune.50.prop=crossover.proportion
#parametertuner.tune.50.type=float
#parametertuner.tune.50.adjust.type=factor
#parametertuner.tune.50.adjust.amount=2
#parametertuner.tune.50.initial=0.5
#parametertuner.tune.50.min=0
#parametertuner.tune.50.max=1

#parametertuner.tune.60.prop=crossover.mutate.probability
#parametertuner.tune.60.type=float
#parametertuner.tune.60.adjust.type=factor
#parametertuner.tune.60.adjust.amount=2
#parametertuner.tune.60.initial=0.5
#parametertuner.tune.60.min=0
#parametertuner.tune.60.max=1

#parametertuner.tune.70.prop=fitness.function.rlcss.noveltysearch.even_distribution.record_count
#parametertuner.tune.70.type=integer
#parametertuner.tune.70.adjust.type=factor
#parametertuner.tune.70.adjust.amount=2
#parametertuner.tune.70.initial=2
#parametertuner.tune.70.min=1
#parametertuner.tune.70.max=8


parametertuner.numruns=75
parametertuner.numgens=1000
parametertuner.solvedperformance=0.98
parametertuner.htcondor=\
  jar_files = ../../../lib/aparapi.jar ../../../lib/bain.jar ../../../lib/commons-lang3-3.1.jar ../../../lib/commons-math3-3.1.1.jar ../../../lib/jakarta-regexp-1.3.jar ../../../lib/jcommander.jar ../../../lib/log4j.jar ../../../lib/wildcard-1.03.jar \n \
  Rank                  = kflops \n \
  +RequiresWholeMachine = True \n \
  notification = Never
